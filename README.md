1. Создание собственного алгоритма сжатия.

Цель: превзойти Huffman, RLE и LZW. 

Чтобы превзойти Huffman, RLE и LZW, нужно определить, для каких данных оптимизируем алгоритм:
- Huffman: Хорошо для текстов с неравномерной частотой символов, но не использует повторяющиеся последовательности.
- RLE: Эффективен для данных с длинными повторами (например, изображения с большими областями одного цвета), но слаб для сложных данных.
- LZW: Отлично для данных с повторяющимися подстроками (например, текст), но словарь может быть неэффективным для случайных данных.

Цель 2: Создать гибридный алгоритм, который:
- Учитывает частоту символов (как Huffman).
- Использует повторы (как RLE).
- Строит адаптивный словарь (как LZW, но более компактный).
- Добавляет предварительную трансформацию для увеличения избыточности (вдохновлённую BWT).

Ниша: Алгоритм будет оптимизирован для текстовых данных (например, логи, исходный код, JSON), но с потенциалом для бинарных файлов.


